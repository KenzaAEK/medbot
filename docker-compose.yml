version: '3.8'

services:
  # 1. Le MOTEUR LLM (Ollama)
  ollama:
    image: ollama/ollama
    container_name: ollama_llm_service
    restart: always
    # IMPORTANT: Assure la persistance du modèle et de ChromaDB sur ton disque
    volumes:
      - C:\ollama\data:/root
    # Expose le port pour l'accès local et interne
    ports:
      - "11434:11434"

  # 2. L'APPLICATION RAG (Ton code Python/LangChain)
  app_rag:
    build: .
    container_name: startguide_ai_app
    # Le conteneur doit attendre qu'Ollama soit démarré
    depends_on:
      - ollama
    # Assure que le code peut voir et accéder au volume de données
    volumes:
      - ./app:/app/app       # Monte le code local pour faciliter le développement
      - ./data:/app/data
      - C:\ollama\data:/root/data  # Point d'accès à la BDD Chroma
    # Commande de démarrage (on utilisera uvicorn pour FastAPI plus tard)
    command: ["tail", "-f", "/dev/null"] # Conserve le conteneur actif pour le moment
    environment:
      # Variable d'environnement pour que LangChain sache où trouver Ollama
      - OLLAMA_HOST=http://ollama:11434 # 'ollama' est le nom du service Docker