version: '3.8'

services:
  # Service 1 : LLM Engine (Ollama)
  ollama:
    image: ollama/ollama:latest
    container_name: medbot_ollama
    restart: always
    volumes:
      - ollama_storage:/root/.ollama  # Volume dédié aux modèles (Mistral)
    ports:
      - "11434:11434"
    networks:
      - rag_network

  # Service 2 : MedBot Streamlit App
  medbot_app:
    build: .
    container_name: medbot_streamlit
    depends_on:
      - ollama
    volumes:
      - ./data:/app/data             # Medical data and ontology
      - ./src:/app/src               # Source code
      - ./app:/app/app               # Streamlit app
      - ./tests:/app/tests           # Tests folder
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - MODEL_NAME=mistral
      - ONTOLOGY_PATH=/app/data/ontology/medical_ontology.ttl
      - DATA_PATH=/app/data/processed/consolidated_medical_data.json
      - DEFAULT_LANGUAGE=fr
    ports:
      - "8501:8501"
    command: ["streamlit", "run", "app/main.py", "--server.port=8501", "--server.address=0.0.0.0"]
    networks:
      - rag_network

# Définition des volumes persistants
volumes:
  ollama_storage:

# Réseau isolé
networks:
  rag_network:
    driver: bridge